{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988ee84-c0ba-498a-9bc7-340688523fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 2.3043 | Grad Norm (after clip): 0.4310\n",
      "Epoch 2 | Loss: 2.2896 | Grad Norm (after clip): 0.4094\n",
      "Epoch 3 | Loss: 2.2765 | Grad Norm (after clip): 0.4380\n",
      "Epoch 4 | Loss: 2.2627 | Grad Norm (after clip): 0.4674\n",
      "Epoch 5 | Loss: 2.2475 | Grad Norm (after clip): 0.5040\n",
      "Epoch 6 | Loss: 2.2298 | Grad Norm (after clip): 0.5611\n",
      "Epoch 7 | Loss: 2.2090 | Grad Norm (after clip): 0.6315\n",
      "Epoch 8 | Loss: 2.1840 | Grad Norm (after clip): 0.6922\n",
      "Epoch 9 | Loss: 2.1542 | Grad Norm (after clip): 0.7586\n",
      "Epoch 10 | Loss: 2.1194 | Grad Norm (after clip): 0.8092\n",
      "Epoch 11 | Loss: 2.0801 | Grad Norm (after clip): 0.8509\n",
      "Epoch 12 | Loss: 2.0365 | Grad Norm (after clip): 0.8805\n",
      "Epoch 13 | Loss: 1.9885 | Grad Norm (after clip): 0.9280\n",
      "Epoch 14 | Loss: 1.9363 | Grad Norm (after clip): 1.0059\n",
      "Epoch 15 | Loss: 1.8799 | Grad Norm (after clip): 1.1144\n",
      "Epoch 16 | Loss: 1.8177 | Grad Norm (after clip): 1.2684\n",
      "Epoch 17 | Loss: 1.7486 | Grad Norm (after clip): 1.3961\n",
      "Epoch 18 | Loss: 1.6720 | Grad Norm (after clip): 1.5304\n",
      "Epoch 19 | Loss: 1.5871 | Grad Norm (after clip): 1.6184\n",
      "Epoch 20 | Loss: 1.4958 | Grad Norm (after clip): 1.6587\n",
      "Epoch 21 | Loss: 1.3996 | Grad Norm (after clip): 1.6845\n",
      "Epoch 22 | Loss: 1.3002 | Grad Norm (after clip): 1.6984\n",
      "Epoch 23 | Loss: 1.2004 | Grad Norm (after clip): 1.6932\n",
      "Epoch 24 | Loss: 1.1021 | Grad Norm (after clip): 1.6951\n",
      "Epoch 25 | Loss: 1.0051 | Grad Norm (after clip): 1.6639\n",
      "Epoch 26 | Loss: 0.9111 | Grad Norm (after clip): 1.6500\n",
      "Epoch 27 | Loss: 0.8211 | Grad Norm (after clip): 1.6135\n",
      "Epoch 28 | Loss: 0.7357 | Grad Norm (after clip): 1.5732\n",
      "Epoch 29 | Loss: 0.6549 | Grad Norm (after clip): 1.5212\n",
      "Epoch 30 | Loss: 0.5788 | Grad Norm (after clip): 1.4787\n",
      "Epoch 31 | Loss: 0.5070 | Grad Norm (after clip): 1.4552\n",
      "Epoch 32 | Loss: 0.4388 | Grad Norm (after clip): 1.4251\n",
      "Epoch 33 | Loss: 0.3744 | Grad Norm (after clip): 1.3599\n",
      "Epoch 34 | Loss: 0.3144 | Grad Norm (after clip): 1.2825\n",
      "Epoch 35 | Loss: 0.2600 | Grad Norm (after clip): 1.2130\n",
      "Epoch 36 | Loss: 0.2114 | Grad Norm (after clip): 1.1229\n",
      "Epoch 37 | Loss: 0.1687 | Grad Norm (after clip): 0.9955\n",
      "Epoch 38 | Loss: 0.1324 | Grad Norm (after clip): 0.8341\n",
      "Epoch 39 | Loss: 0.1036 | Grad Norm (after clip): 0.7318\n",
      "Epoch 40 | Loss: 0.0817 | Grad Norm (after clip): 0.7948\n",
      "Epoch 41 | Loss: 0.0638 | Grad Norm (after clip): 0.8633\n",
      "Epoch 42 | Loss: 0.0476 | Grad Norm (after clip): 0.7287\n",
      "Epoch 43 | Loss: 0.0340 | Grad Norm (after clip): 0.5255\n",
      "Epoch 44 | Loss: 0.0240 | Grad Norm (after clip): 0.3886\n",
      "Epoch 45 | Loss: 0.0168 | Grad Norm (after clip): 0.2895\n",
      "Epoch 46 | Loss: 0.0119 | Grad Norm (after clip): 0.2206\n",
      "Epoch 47 | Loss: 0.0085 | Grad Norm (after clip): 0.1809\n",
      "Epoch 48 | Loss: 0.0063 | Grad Norm (after clip): 0.1566\n",
      "Epoch 49 | Loss: 0.0047 | Grad Norm (after clip): 0.1335\n",
      "Epoch 50 | Loss: 0.0036 | Grad Norm (after clip): 0.1112\n",
      "Epoch 51 | Loss: 0.0028 | Grad Norm (after clip): 0.0925\n",
      "Epoch 52 | Loss: 0.0022 | Grad Norm (after clip): 0.0782\n",
      "Epoch 53 | Loss: 0.0018 | Grad Norm (after clip): 0.0673\n",
      "Epoch 54 | Loss: 0.0014 | Grad Norm (after clip): 0.0581\n",
      "Epoch 55 | Loss: 0.0012 | Grad Norm (after clip): 0.0502\n",
      "Epoch 56 | Loss: 0.0010 | Grad Norm (after clip): 0.0431\n",
      "Epoch 57 | Loss: 0.0008 | Grad Norm (after clip): 0.0370\n",
      "Epoch 58 | Loss: 0.0007 | Grad Norm (after clip): 0.0318\n",
      "Epoch 59 | Loss: 0.0006 | Grad Norm (after clip): 0.0274\n",
      "Epoch 60 | Loss: 0.0005 | Grad Norm (after clip): 0.0238\n",
      "Epoch 61 | Loss: 0.0004 | Grad Norm (after clip): 0.0208\n",
      "Epoch 62 | Loss: 0.0004 | Grad Norm (after clip): 0.0184\n",
      "Epoch 63 | Loss: 0.0003 | Grad Norm (after clip): 0.0163\n",
      "Epoch 64 | Loss: 0.0003 | Grad Norm (after clip): 0.0145\n",
      "Epoch 65 | Loss: 0.0003 | Grad Norm (after clip): 0.0130\n",
      "Epoch 66 | Loss: 0.0002 | Grad Norm (after clip): 0.0117\n",
      "Epoch 67 | Loss: 0.0002 | Grad Norm (after clip): 0.0106\n",
      "Epoch 68 | Loss: 0.0002 | Grad Norm (after clip): 0.0096\n",
      "Epoch 69 | Loss: 0.0002 | Grad Norm (after clip): 0.0087\n",
      "Epoch 70 | Loss: 0.0002 | Grad Norm (after clip): 0.0080\n",
      "Epoch 71 | Loss: 0.0001 | Grad Norm (after clip): 0.0073\n",
      "Epoch 72 | Loss: 0.0001 | Grad Norm (after clip): 0.0067\n",
      "Epoch 73 | Loss: 0.0001 | Grad Norm (after clip): 0.0061\n",
      "Epoch 74 | Loss: 0.0001 | Grad Norm (after clip): 0.0056\n",
      "Epoch 75 | Loss: 0.0001 | Grad Norm (after clip): 0.0052\n",
      "Epoch 76 | Loss: 0.0001 | Grad Norm (after clip): 0.0048\n",
      "Epoch 77 | Loss: 0.0001 | Grad Norm (after clip): 0.0045\n",
      "Epoch 78 | Loss: 0.0001 | Grad Norm (after clip): 0.0041\n",
      "Epoch 79 | Loss: 0.0001 | Grad Norm (after clip): 0.0039\n",
      "Epoch 80 | Loss: 0.0001 | Grad Norm (after clip): 0.0036\n",
      "Epoch 81 | Loss: 0.0001 | Grad Norm (after clip): 0.0034\n",
      "Epoch 82 | Loss: 0.0001 | Grad Norm (after clip): 0.0032\n",
      "Epoch 83 | Loss: 0.0001 | Grad Norm (after clip): 0.0030\n",
      "Epoch 84 | Loss: 0.0001 | Grad Norm (after clip): 0.0028\n",
      "Epoch 85 | Loss: 0.0001 | Grad Norm (after clip): 0.0026\n",
      "Epoch 86 | Loss: 0.0001 | Grad Norm (after clip): 0.0025\n",
      "Epoch 87 | Loss: 0.0001 | Grad Norm (after clip): 0.0024\n",
      "Epoch 88 | Loss: 0.0001 | Grad Norm (after clip): 0.0023\n",
      "Epoch 89 | Loss: 0.0001 | Grad Norm (after clip): 0.0021\n",
      "Epoch 90 | Loss: 0.0001 | Grad Norm (after clip): 0.0021\n",
      "Epoch 91 | Loss: 0.0001 | Grad Norm (after clip): 0.0020\n",
      "Epoch 92 | Loss: 0.0001 | Grad Norm (after clip): 0.0019\n",
      "Epoch 93 | Loss: 0.0001 | Grad Norm (after clip): 0.0018\n",
      "Epoch 94 | Loss: 0.0001 | Grad Norm (after clip): 0.0018\n",
      "Epoch 95 | Loss: 0.0001 | Grad Norm (after clip): 0.0017\n",
      "Epoch 96 | Loss: 0.0001 | Grad Norm (after clip): 0.0017\n",
      "Epoch 97 | Loss: 0.0001 | Grad Norm (after clip): 0.0016\n",
      "Epoch 98 | Loss: 0.0001 | Grad Norm (after clip): 0.0016\n",
      "Epoch 99 | Loss: 0.0001 | Grad Norm (after clip): 0.0016\n",
      "Epoch 100 | Loss: 0.0001 | Grad Norm (after clip): 0.0015\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(100, 124),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(124, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 124),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(124, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters())\n",
    "\n",
    "x = torch.randn(64, 100)\n",
    "y = torch.randint(0, 10, (64,))\n",
    "\n",
    "for i in range(100):\n",
    "    model.train()\n",
    "    y_preds = model(x)\n",
    "    loss = criterion(y_preds, y)\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "    optimiser.step()\n",
    "\n",
    "    grad_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_norm += p.grad.data.norm(2).item()\n",
    "    print(\n",
    "        f\"Epoch {i+1} | Loss: {loss.item():.4f} | Grad Norm (after clip): {grad_norm:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
